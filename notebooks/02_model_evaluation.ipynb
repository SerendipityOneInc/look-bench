{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LookBench Model Evaluation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SerendipityOneInc/look-bench/blob/main/notebooks/02_model_evaluation.ipynb)\n",
        "\n",
        "This notebook demonstrates how to perform **complete model evaluation** on the LookBench dataset with all metrics.\n",
        "\n",
        "We'll evaluate models using:\n",
        "- **Recall@K** (K=1, 5, 10, 20)\n",
        "- **MRR** (Mean Reciprocal Rank)\n",
        "- **NDCG@5** (Normalized Discounted Cumulative Gain)\n",
        "- **mAP** (Mean Average Precision)\n",
        "\n",
        "\ud83d\udcc4 **Paper**: [arxiv.org/abs/2601.14706](https://arxiv.org/abs/2601.14706)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch torchvision transformers datasets pillow pandas pyarrow pyyaml tqdm matplotlib\n",
        "\n",
        "# Clone LookBench repository\n",
        "!git clone https://github.com/SerendipityOneInc/look-bench.git\n",
        "%cd look-bench\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/look-bench')\n",
        "\n",
        "print(\"\u2705 Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from manager import ConfigManager, ModelManager\n",
        "from metrics import RankEvaluator, MRREvaluator, NDCGEvaluator, MAPEvaluator\n",
        "\n",
        "print(\"\u2705 Imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load LookBench Dataset\n",
        "\n",
        "Choose which subset to evaluate:\n",
        "- `real_studio_flat` - Easy: single-item retrieval\n",
        "- `aigen_studio` - Medium: AI-generated studio images\n",
        "- `real_streetlook` - Hard: multi-item outfit retrieval\n",
        "- `aigen_streetlook` - Hard: AI-generated street looks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset from Hugging Face\n",
        "print(\"Loading LookBench dataset...\")\n",
        "dataset = load_dataset(\"srpone/look-bench\")\n",
        "\n",
        "# Choose subset to evaluate\n",
        "subset_name = 'real_studio_flat'  # Change this to evaluate other subsets\n",
        "\n",
        "query_data = dataset[subset_name]['query']\n",
        "gallery_data = dataset[subset_name]['gallery']\n",
        "\n",
        "print(f\"\\n\u2705 Evaluating on: {subset_name}\")\n",
        "print(f\"   Query samples: {len(query_data)}\")\n",
        "print(f\"   Gallery samples: {len(gallery_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model\n",
        "\n",
        "Available models: `clip`, `siglip`, `dinov2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize managers\n",
        "config_manager = ConfigManager('configs/config.yaml')\n",
        "model_manager = ModelManager(config_manager)\n",
        "\n",
        "# Choose model to evaluate\n",
        "model_name = 'clip'  # Change to 'siglip' or 'dinov2' to test other models\n",
        "\n",
        "# Load model\n",
        "print(f\"Loading {model_name} model...\")\n",
        "model, model_wrapper = model_manager.load_model(model_name)\n",
        "transform = model_manager.get_transform(model_name)\n",
        "\n",
        "model.eval()\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    print(\"\u2705 Model moved to CUDA\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Running on CPU (slower)\")\n",
        "\n",
        "print(f\"\u2705 Model loaded: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Features\n",
        "\n",
        "This may take a few minutes depending on dataset size and GPU availability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features_from_dataset(data, model, transform, batch_size=32):\n",
        "    \"\"\"Extract features from a dataset\"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(data), batch_size), desc=\"Extracting features\"):\n",
        "            batch_data = data[i:i+batch_size]\n",
        "            \n",
        "            batch_images = []\n",
        "            batch_labels = []\n",
        "            \n",
        "            for idx, sample in enumerate(batch_data):\n",
        "                img = sample['image']\n",
        "                img_tensor = transform(img)\n",
        "                batch_images.append(img_tensor)\n",
        "                \n",
        "                # Use item_ID as label if available, otherwise use index\n",
        "                label = sample.get('item_ID', i + idx)\n",
        "                batch_labels.append(label)\n",
        "            \n",
        "            # Stack batch\n",
        "            batch_tensor = torch.stack(batch_images)\n",
        "            if torch.cuda.is_available():\n",
        "                batch_tensor = batch_tensor.cuda()\n",
        "            \n",
        "            # Extract features\n",
        "            batch_features = model(batch_tensor)\n",
        "            features.append(batch_features.cpu())\n",
        "            labels.extend(batch_labels)\n",
        "    \n",
        "    features = torch.cat(features, dim=0)\n",
        "    return features.numpy(), np.array(labels)\n",
        "\n",
        "# Extract features\n",
        "print(\"\\n\ud83d\udcca Extracting query features...\")\n",
        "query_features, query_labels = extract_features_from_dataset(query_data, model, transform)\n",
        "\n",
        "print(\"\\n\ud83d\udcca Extracting gallery features...\")\n",
        "gallery_features, gallery_labels = extract_features_from_dataset(gallery_data, model, transform)\n",
        "\n",
        "print(f\"\\n\u2705 Feature extraction complete!\")\n",
        "print(f\"   Query features shape: {query_features.shape}\")\n",
        "print(f\"   Gallery features shape: {gallery_features.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L2 Normalization\n",
        "\n",
        "Normalize features for cosine similarity computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# L2 normalize features\n",
        "query_features = query_features / np.linalg.norm(query_features, axis=1, keepdims=True)\n",
        "gallery_features = gallery_features / np.linalg.norm(gallery_features, axis=1, keepdims=True)\n",
        "\n",
        "print(\"\u2705 Features normalized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Similarity Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute cosine similarity\n",
        "similarity_matrix = np.dot(query_features, gallery_features.T)\n",
        "sorted_indices = np.argsort(-similarity_matrix, axis=1)\n",
        "\n",
        "print(f\"\u2705 Similarity matrix computed\")\n",
        "print(f\"   Shape: {similarity_matrix.shape}\")\n",
        "print(f\"   Range: [{similarity_matrix.min():.4f}, {similarity_matrix.max():.4f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate with All Metrics\n",
        "\n",
        "Computing Recall@K, MRR, NDCG@5, and mAP..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluators\n",
        "rank_evaluator = RankEvaluator(top_k=[1, 5, 10, 20])\n",
        "mrr_evaluator = MRREvaluator()\n",
        "ndcg_evaluator = NDCGEvaluator(k=5)\n",
        "map_evaluator = MAPEvaluator()\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Recall@K\n",
        "print(\"Computing Recall@K...\")\n",
        "for k in [1, 5, 10, 20]:\n",
        "    scores = []\n",
        "    for i in range(len(query_labels)):\n",
        "        score = rank_evaluator.metric_eval(\n",
        "            sorted_indices[i],\n",
        "            k,\n",
        "            query_labels[i],\n",
        "            gallery_labels\n",
        "        )\n",
        "        scores.append(score)\n",
        "    results[f'Recall@{k}'] = np.mean(scores) * 100\n",
        "\n",
        "# MRR\n",
        "print(\"Computing MRR...\")\n",
        "mrr_scores = []\n",
        "for i in range(len(query_labels)):\n",
        "    score = mrr_evaluator.metric_eval(\n",
        "        sorted_indices[i],\n",
        "        None,\n",
        "        query_labels[i],\n",
        "        gallery_labels\n",
        "    )\n",
        "    mrr_scores.append(score)\n",
        "results['MRR'] = np.mean(mrr_scores) * 100\n",
        "\n",
        "# NDCG@5\n",
        "print(\"Computing NDCG@5...\")\n",
        "ndcg_scores = []\n",
        "for i in range(len(query_labels)):\n",
        "    score = ndcg_evaluator.metric_eval(\n",
        "        sorted_indices[i],\n",
        "        5,\n",
        "        query_labels[i],\n",
        "        gallery_labels\n",
        "    )\n",
        "    ndcg_scores.append(score)\n",
        "results['NDCG@5'] = np.mean(ndcg_scores) * 100\n",
        "\n",
        "# mAP\n",
        "print(\"Computing mAP...\")\n",
        "map_scores = []\n",
        "for i in range(len(query_labels)):\n",
        "    score = map_evaluator.metric_eval(\n",
        "        sorted_indices[i],\n",
        "        None,\n",
        "        query_labels[i],\n",
        "        gallery_labels\n",
        "    )\n",
        "    map_scores.append(score)\n",
        "results['mAP'] = np.mean(map_scores) * 100\n",
        "\n",
        "print(\"\\n\u2705 All metrics computed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print results\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Evaluation Results on {subset_name}\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"{'='*60}\")\n",
        "for metric, value in results.items():\n",
        "    print(f\"{metric:15s}: {value:6.2f}%\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Top Retrievals\n",
        "\n",
        "Let's visualize some retrieval results to understand model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_retrieval(query_idx, top_k=5):\n",
        "    \"\"\"Visualize top-k retrievals for a query\"\"\"\n",
        "    query_sample = query_data[query_idx]\n",
        "    top_indices = sorted_indices[query_idx][:top_k]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, top_k + 1, figsize=(4 * (top_k + 1), 4))\n",
        "    \n",
        "    # Show query\n",
        "    axes[0].imshow(query_sample['image'])\n",
        "    axes[0].set_title('Query\\n' + f\"Category: {query_sample.get('category', 'N/A')}\\n\" +\n",
        "                     f\"Label: {query_labels[query_idx]}\", \n",
        "                     fontsize=10, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Show top retrievals\n",
        "    for i, gallery_idx in enumerate(top_indices):\n",
        "        gallery_sample = gallery_data[gallery_idx]\n",
        "        sim = similarity_matrix[query_idx, gallery_idx]\n",
        "        match = gallery_labels[gallery_idx] == query_labels[query_idx]\n",
        "        \n",
        "        axes[i + 1].imshow(gallery_sample['image'])\n",
        "        title = f\"Rank {i+1}\\nSim: {sim:.3f}\\nLabel: {gallery_labels[gallery_idx]}\"\n",
        "        if match:\n",
        "            title += \"\\n\u2713 MATCH\"\n",
        "            axes[i + 1].set_title(title, fontsize=10, color='green', fontweight='bold')\n",
        "        else:\n",
        "            title += \"\\n\u2717 NO MATCH\"\n",
        "            axes[i + 1].set_title(title, fontsize=10, color='red')\n",
        "        axes[i + 1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize a few examples\n",
        "for i in range(min(3, len(query_data))):\n",
        "    visualize_retrieval(i, top_k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on Multiple Subsets (Optional)\n",
        "\n",
        "Uncomment and run this cell to evaluate on all LookBench subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Evaluate on all subsets\n",
        "# all_results = {}\n",
        "# subsets = ['real_studio_flat', 'aigen_studio', 'real_streetlook', 'aigen_streetlook']\n",
        "# \n",
        "# for subset in subsets:\n",
        "#     print(f\"\\n{'='*60}\")\n",
        "#     print(f\"Evaluating on {subset}...\")\n",
        "#     print(f\"{'='*60}\")\n",
        "#     \n",
        "#     # Load subset\n",
        "#     query_data_sub = dataset[subset]['query']\n",
        "#     gallery_data_sub = dataset[subset]['gallery']\n",
        "#     \n",
        "#     # Extract features and evaluate\n",
        "#     # ... (copy the feature extraction and evaluation code here)\n",
        "#     \n",
        "#     all_results[subset] = results\n",
        "# \n",
        "# # Print summary\n",
        "# import pandas as pd\n",
        "# df = pd.DataFrame(all_results).T\n",
        "# print(\"\\n\" + \"=\"*60)\n",
        "# print(\"Summary Across All Subsets\")\n",
        "# print(\"=\"*60)\n",
        "# print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Try different models**: Change `model_name` to 'siglip' or 'dinov2'\n",
        "2. **Evaluate other subsets**: Change `subset_name` to test different difficulty levels\n",
        "3. **Integrate custom models**: See `03_custom_model.ipynb`\n",
        "4. **Fine-tune models**: Use LookBench for model training and evaluation\n",
        "\n",
        "### Useful Links\n",
        "\n",
        "- \ud83d\udcc4 **Paper**: https://arxiv.org/abs/2601.14706\n",
        "- \ud83c\udfe0 **Project**: https://serendipityoneinc.github.io/look-bench-page/\n",
        "- \ud83e\udd17 **Dataset**: https://huggingface.co/datasets/srpone/look-bench\n",
        "- \ud83d\udcbb **GitHub**: https://github.com/SerendipityOneInc/look-bench"
      ]
    }
  ]
}