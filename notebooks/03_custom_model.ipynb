{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Integrating Custom Models with LookBench\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SerendipityOneInc/look-bench/blob/main/notebooks/03_custom_model.ipynb)\n",
        "\n",
        "This notebook demonstrates how to **integrate your own custom models** into the LookBench framework using the registry pattern.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. Create a custom model class inheriting from `BaseModel`\n",
        "2. Register your model using the `@register_model` decorator\n",
        "3. Implement required methods (`load_model`, `get_transform`)\n",
        "4. Test your model with LookBench dataset\n",
        "5. Integrate Hugging Face models\n",
        "\n",
        "\ud83d\udcc4 **Paper**: [arxiv.org/abs/2601.14706](https://arxiv.org/abs/2601.14706)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch torchvision transformers datasets pillow pandas pyarrow pyyaml tqdm matplotlib\n",
        "\n",
        "# Clone LookBench repository\n",
        "!git clone https://github.com/SerendipityOneInc/look-bench.git\n",
        "%cd look-bench\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/look-bench')\n",
        "\n",
        "print(\"\u2705 Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "from models.base import BaseModel\n",
        "from models.registry import register_model, list_available_models\n",
        "\n",
        "print(\"\u2705 Imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: ResNet-50 Model\n",
        "\n",
        "Let's create a simple ResNet-50 model for fashion image retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@register_model(\"resnet50\", metadata={\n",
        "    \"description\": \"ResNet-50 pretrained on ImageNet\",\n",
        "    \"framework\": \"PyTorch\",\n",
        "    \"input_size\": 224,\n",
        "    \"embedding_dim\": 2048\n",
        "})\n",
        "class ResNet50Model(BaseModel):\n",
        "    \"\"\"ResNet-50 model for image embedding extraction\"\"\"\n",
        "    \n",
        "    @classmethod\n",
        "    def load_model(cls, model_name: str = \"resnet50\", model_path: str = None):\n",
        "        \"\"\"\n",
        "        Load ResNet-50 model\n",
        "        \n",
        "        Args:\n",
        "            model_name: Name of the model\n",
        "            model_path: Optional path to custom weights\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (model, wrapper_instance)\n",
        "        \"\"\"\n",
        "        # Load pretrained ResNet-50\n",
        "        model = models.resnet50(pretrained=True)\n",
        "        \n",
        "        # Remove classification head, keep feature extractor\n",
        "        model = nn.Sequential(*list(model.children())[:-1])\n",
        "        \n",
        "        # Load custom weights if provided\n",
        "        if model_path:\n",
        "            state_dict = torch.load(model_path, map_location='cpu')\n",
        "            model.load_state_dict(state_dict)\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        # Create a wrapper that flattens the output\n",
        "        class ModelWrapper(nn.Module):\n",
        "            def __init__(self, backbone):\n",
        "                super().__init__()\n",
        "                self.backbone = backbone\n",
        "            \n",
        "            def forward(self, x):\n",
        "                features = self.backbone(x)\n",
        "                return features.squeeze(-1).squeeze(-1)  # Flatten spatial dimensions\n",
        "        \n",
        "        wrapped_model = ModelWrapper(model)\n",
        "        \n",
        "        return wrapped_model, cls()\n",
        "    \n",
        "    @classmethod\n",
        "    def get_transform(cls, input_size: int = 224):\n",
        "        \"\"\"\n",
        "        Get image preprocessing transform\n",
        "        \n",
        "        Args:\n",
        "            input_size: Input image size\n",
        "            \n",
        "        Returns:\n",
        "            torchvision.transforms composition\n",
        "        \"\"\"\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ])\n",
        "\n",
        "print(\"\u2705 ResNet-50 model registered!\")\n",
        "print(f\"Available models: {list_available_models()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Custom Model\n",
        "\n",
        "Let's load the model and test it with a sample image from LookBench."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model\n",
        "print(\"Loading ResNet-50 model...\")\n",
        "model, wrapper = ResNet50Model.load_model()\n",
        "transform = ResNet50Model.get_transform()\n",
        "\n",
        "# Move to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    print(\"\u2705 Model on CUDA\")\n",
        "\n",
        "# Load sample image\n",
        "dataset = load_dataset(\"srpone/look-bench\")\n",
        "sample_img = dataset['real_studio_flat']['query'][0]['image']\n",
        "\n",
        "# Preprocess and extract features\n",
        "img_tensor = transform(sample_img).unsqueeze(0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    img_tensor = img_tensor.cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    features = model(img_tensor)\n",
        "\n",
        "print(f\"\\n\u2705 Feature extraction successful!\")\n",
        "print(f\"   Feature shape: {features.shape}\")\n",
        "print(f\"   Feature dimension: {features.shape[1]}\")\n",
        "print(f\"   Feature norm: {torch.norm(features).item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Custom Architecture\n",
        "\n",
        "Here's how to integrate a model with custom architecture and checkpoint loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@register_model(\"custom_fashion_model\", metadata={\n",
        "    \"description\": \"Custom fashion embedding model\",\n",
        "    \"framework\": \"PyTorch\",\n",
        "    \"input_size\": 256,\n",
        "    \"embedding_dim\": 512\n",
        "})\n",
        "class CustomFashionModel(BaseModel):\n",
        "    \"\"\"Custom fashion model trained on fashion data\"\"\"\n",
        "    \n",
        "    @classmethod\n",
        "    def load_model(cls, model_name: str = \"custom_fashion_model\", model_path: str = None):\n",
        "        \"\"\"Load custom model\"\"\"\n",
        "        # Define your model architecture\n",
        "        class YourCustomArchitecture(nn.Module):\n",
        "            def __init__(self, embedding_dim=512):\n",
        "                super().__init__()\n",
        "                # Your model architecture here\n",
        "                self.backbone = models.resnet34(pretrained=True)\n",
        "                self.backbone.fc = nn.Linear(self.backbone.fc.in_features, embedding_dim)\n",
        "            \n",
        "            def forward(self, x):\n",
        "                return self.backbone(x)\n",
        "        \n",
        "        # Instantiate model\n",
        "        model = YourCustomArchitecture(embedding_dim=512)\n",
        "        \n",
        "        # Load trained weights if provided\n",
        "        if model_path:\n",
        "            checkpoint = torch.load(model_path, map_location='cpu')\n",
        "            \n",
        "            # Handle different checkpoint formats\n",
        "            if isinstance(checkpoint, dict):\n",
        "                if 'model_state_dict' in checkpoint:\n",
        "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                elif 'state_dict' in checkpoint:\n",
        "                    model.load_state_dict(checkpoint['state_dict'])\n",
        "                else:\n",
        "                    model.load_state_dict(checkpoint)\n",
        "            else:\n",
        "                model.load_state_dict(checkpoint)\n",
        "        \n",
        "        model.eval()\n",
        "        return model, cls()\n",
        "    \n",
        "    @classmethod\n",
        "    def get_transform(cls, input_size: int = 256):\n",
        "        \"\"\"Custom preprocessing pipeline\"\"\"\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ])\n",
        "\n",
        "print(\"\u2705 Custom fashion model registered!\")\n",
        "print(f\"Available models: {list_available_models()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Hugging Face Models\n",
        "\n",
        "Integrate models from Hugging Face Hub (like GR-Lite)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoImageProcessor\n",
        "\n",
        "@register_model(\"gr_lite\", metadata={\n",
        "    \"description\": \"GR-Lite model from srpone\",\n",
        "    \"framework\": \"PyTorch/Transformers\",\n",
        "    \"input_size\": 336,\n",
        "    \"embedding_dim\": 1024\n",
        "})\n",
        "class GRLiteModel(BaseModel):\n",
        "    \"\"\"GR-Lite model for fashion retrieval\"\"\"\n",
        "    \n",
        "    @classmethod\n",
        "    def load_model(cls, model_name: str = \"srpone/gr-lite\", model_path: str = None):\n",
        "        \"\"\"Load GR-Lite model from Hugging Face\"\"\"\n",
        "        # Load model from Hugging Face Hub or local path\n",
        "        if model_path:\n",
        "            model = AutoModel.from_pretrained(model_path)\n",
        "        else:\n",
        "            model = AutoModel.from_pretrained(model_name)\n",
        "        \n",
        "        model.eval()\n",
        "        return model, cls()\n",
        "    \n",
        "    @classmethod\n",
        "    def get_transform(cls, input_size: int = 336):\n",
        "        \"\"\"Get transform for GR-Lite\"\"\"\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ])\n",
        "\n",
        "print(\"\u2705 GR-Lite model registered!\")\n",
        "print(f\"\\nAll available models: {list_available_models()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Similarity Computation\n",
        "\n",
        "Let's test the custom model with similarity computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load ResNet-50 model\n",
        "model, _ = ResNet50Model.load_model()\n",
        "transform = ResNet50Model.get_transform()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "# Load dataset\n",
        "query_img = dataset['real_studio_flat']['query'][0]['image']\n",
        "gallery_imgs = [dataset['real_studio_flat']['gallery'][i]['image'] for i in range(3)]\n",
        "\n",
        "def extract_features(images, model, transform):\n",
        "    \"\"\"Extract features from images\"\"\"\n",
        "    features = []\n",
        "    for img in images:\n",
        "        img_tensor = transform(img).unsqueeze(0)\n",
        "        if torch.cuda.is_available():\n",
        "            img_tensor = img_tensor.cuda()\n",
        "        with torch.no_grad():\n",
        "            feat = model(img_tensor)\n",
        "        features.append(feat.cpu().numpy())\n",
        "    return np.vstack(features)\n",
        "\n",
        "# Extract features\n",
        "query_feat = extract_features([query_img], model, transform)\n",
        "gallery_feats = extract_features(gallery_imgs, model, transform)\n",
        "\n",
        "# Compute cosine similarity\n",
        "query_norm = query_feat / np.linalg.norm(query_feat, axis=1, keepdims=True)\n",
        "gallery_norm = gallery_feats / np.linalg.norm(gallery_feats, axis=1, keepdims=True)\n",
        "similarities = np.dot(query_norm, gallery_norm.T)[0]\n",
        "\n",
        "print(\"\u2705 Similarity computation:\")\n",
        "for i, sim in enumerate(similarities):\n",
        "    print(f\"   Gallery image {i}: {sim:.4f}\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "axes[0].imshow(query_img)\n",
        "axes[0].set_title('Query', fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "for i, (img, sim) in enumerate(zip(gallery_imgs, similarities)):\n",
        "    axes[i+1].imshow(img)\n",
        "    axes[i+1].set_title(f'Gallery {i}\\nSim: {sim:.3f}')\n",
        "    axes[i+1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\u2705 Custom model integration test passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Your Model\n",
        "\n",
        "To use your custom model with LookBench's main evaluation pipeline, add it to `configs/config.yaml`:\n",
        "\n",
        "```yaml\n",
        "# Custom ResNet-50 model\n",
        "resnet50:\n",
        "  enabled: true\n",
        "  model_name: \"resnet50\"\n",
        "  model_path: null\n",
        "  input_size: 224\n",
        "  embedding_dim: 2048\n",
        "  device: \"cuda\"\n",
        "\n",
        "# Custom trained model\n",
        "custom_fashion_model:\n",
        "  enabled: true\n",
        "  model_name: \"custom_fashion_model\"\n",
        "  model_path: \"/path/to/your/model/weights.pth\"\n",
        "  input_size: 256\n",
        "  embedding_dim: 512\n",
        "  device: \"cuda\"\n",
        "\n",
        "# GR-Lite from Hugging Face\n",
        "gr_lite:\n",
        "  enabled: true\n",
        "  model_name: \"srpone/gr-lite\"\n",
        "  model_path: null\n",
        "  input_size: 336\n",
        "  embedding_dim: 1024\n",
        "  device: \"cuda\"\n",
        "```\n",
        "\n",
        "Then you can run evaluation:\n",
        "```bash\n",
        "python main.py --model resnet50\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Evaluation with Custom Model\n",
        "\n",
        "You can now use your custom model with the full evaluation pipeline from notebook 02."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Quick evaluation with custom model\n",
        "print(\"You can now use your custom model for full evaluation!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  1. Use your model with ConfigManager and ModelManager\")\n",
        "print(\"  2. Run full evaluation from notebook 02\")\n",
        "print(\"  3. Compare with baseline models\")\n",
        "print(\"  4. Submit results to LookBench leaderboard\")\n",
        "\n",
        "print(\"\\n\u2705 All examples completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### 1. Feature Normalization\n",
        "- Always L2-normalize features for retrieval\n",
        "- Improves cosine similarity computation\n",
        "\n",
        "### 2. Input Preprocessing\n",
        "- Use the same preprocessing as training\n",
        "- Match normalization statistics to your model\n",
        "\n",
        "### 3. Model Checkpoints\n",
        "- Handle different checkpoint formats\n",
        "- Support both full models and state_dict\n",
        "\n",
        "### 4. Device Management\n",
        "- Always check CUDA availability\n",
        "- Move tensors to same device as model\n",
        "\n",
        "### 5. Evaluation Mode\n",
        "- Set model to `.eval()` mode\n",
        "- Use `torch.no_grad()` for inference\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Evaluate your model**: Run full evaluation on all LookBench subsets\n",
        "2. **Compare with baselines**: See how your model compares to CLIP, SigLIP, GR-Lite\n",
        "3. **Fine-tune**: Use LookBench for model training and improvement\n",
        "4. **Submit results**: Share your results with the community\n",
        "\n",
        "### Useful Links\n",
        "\n",
        "- \ud83d\udcc4 **Paper**: https://arxiv.org/abs/2601.14706\n",
        "- \ud83c\udfe0 **Project**: https://serendipityoneinc.github.io/look-bench-page/\n",
        "- \ud83e\udd17 **Dataset**: https://huggingface.co/datasets/srpone/look-bench\n",
        "- \ud83e\udd17 **GR-Lite**: https://huggingface.co/srpone/gr-lite\n",
        "- \ud83d\udcbb **GitHub**: https://github.com/SerendipityOneInc/look-bench\n",
        "\n",
        "**Happy modeling! \ud83d\ude80**"
      ]
    }
  ]
}