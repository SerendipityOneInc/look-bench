{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LookBench Quickstart Guide\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SerendipityOneInc/look-bench/blob/main/notebooks/01_quickstart.ipynb)\n",
        "\n",
        "This notebook demonstrates how to get started with **LookBench** for fashion image retrieval evaluation.\n",
        "\n",
        "\ud83d\udcc4 **Paper**: [LookBench: A Live and Holistic Open Benchmark for Fashion Image Retrieval](https://arxiv.org/abs/2601.14706)\n",
        "\n",
        "\ud83c\udfe0 **Project Page**: [https://serendipityoneinc.github.io/look-bench-page/](https://serendipityoneinc.github.io/look-bench-page/)\n",
        "\n",
        "\ud83e\udd17 **Dataset**: [https://huggingface.co/datasets/srpone/look-bench](https://huggingface.co/datasets/srpone/look-bench)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Installation\n",
        "\n",
        "First, let's install the required packages and clone the LookBench repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch torchvision transformers datasets pillow pandas pyarrow pyyaml tqdm matplotlib\n",
        "\n",
        "# Clone LookBench repository\n",
        "!git clone https://github.com/SerendipityOneInc/look-bench.git\n",
        "%cd look-bench\n",
        "\n",
        "print(\"\u2705 Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the LookBench Dataset from Hugging Face\n",
        "\n",
        "LookBench dataset is hosted on Hugging Face and includes 4 subsets:\n",
        "- **RealStudioFlat**: Easy, single-item retrieval from studio photos\n",
        "- **AIGen-Studio**: Medium, AI-generated studio images\n",
        "- **RealStreetLook**: Hard, multi-item outfit retrieval from street photos\n",
        "- **AIGen-StreetLook**: Hard, AI-generated street outfits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load LookBench dataset from Hugging Face\n",
        "print(\"Loading LookBench dataset...\")\n",
        "dataset = load_dataset(\"srpone/look-bench\")\n",
        "\n",
        "print(f\"\\nAvailable subsets: {list(dataset.keys())}\")\n",
        "print(f\"\\nDataset structure:\")\n",
        "for subset_name in dataset.keys():\n",
        "    print(f\"\\n  {subset_name}:\")\n",
        "    print(f\"    Splits: {list(dataset[subset_name].keys())}\")\n",
        "    for split_name in dataset[subset_name].keys():\n",
        "        print(f\"      {split_name}: {len(dataset[subset_name][split_name])} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Explore a Sample\n",
        "\n",
        "Let's look at a sample from the RealStudioFlat subset to understand the data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get a sample from real_studio_flat query split\n",
        "sample = dataset['real_studio_flat']['query'][0]\n",
        "\n",
        "print(\"Sample keys:\", list(sample.keys()))\n",
        "print(f\"\\nCategory: {sample['category']}\")\n",
        "print(f\"Main attribute: {sample['main_attribute']}\")\n",
        "print(f\"Other attributes: {sample['other_attributes']}\")\n",
        "print(f\"Task: {sample['task']}\")\n",
        "print(f\"Difficulty: {sample['difficulty']}\")\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(sample['image'])\n",
        "plt.axis('off')\n",
        "plt.title(f\"{sample['category']} - {sample['main_attribute']}\", fontsize=14, fontweight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load a Model (CLIP)\n",
        "\n",
        "Now let's load a pre-trained CLIP model for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "sys.path.append('/content/look-bench')\n",
        "\n",
        "from manager import ConfigManager, ModelManager, DataManager\n",
        "\n",
        "# Load configuration\n",
        "config_manager = ConfigManager('configs/config.yaml')\n",
        "\n",
        "# Create model manager\n",
        "model_manager = ModelManager(config_manager)\n",
        "\n",
        "print(\"Available models:\", model_manager.get_available_models())\n",
        "\n",
        "# Load CLIP model\n",
        "print(\"\\nLoading CLIP model...\")\n",
        "model, model_wrapper = model_manager.load_model('clip')\n",
        "transform = model_manager.get_transform('clip')\n",
        "\n",
        "# Set to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Move to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    print(\"\u2705 Model moved to CUDA\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Running on CPU (this will be slower)\")\n",
        "\n",
        "print(\"\u2705 Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Extract Features from a Sample Image\n",
        "\n",
        "Let's extract features from a sample image and see the feature representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features from the sample image\n",
        "sample_image = dataset['real_studio_flat']['query'][0]['image']\n",
        "image_tensor = transform(sample_image).unsqueeze(0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    image_tensor = image_tensor.cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    features = model(image_tensor)\n",
        "    \n",
        "print(f\"Feature shape: {features.shape}\")\n",
        "print(f\"Feature dimension: {features.shape[1]}\")\n",
        "print(f\"Feature norm: {torch.norm(features).item():.4f}\")\n",
        "print(f\"Feature min/max: [{features.min().item():.4f}, {features.max().item():.4f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compute Similarity with Gallery Images\n",
        "\n",
        "Let's compute similarity between a query image and several gallery images to see how retrieval works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def extract_features(images, model, transform):\n",
        "    \"\"\"Extract features from a list of images\"\"\"\n",
        "    features = []\n",
        "    for img in images:\n",
        "        img_tensor = transform(img).unsqueeze(0)\n",
        "        if torch.cuda.is_available():\n",
        "            img_tensor = img_tensor.cuda()\n",
        "        with torch.no_grad():\n",
        "            feat = model(img_tensor)\n",
        "        features.append(feat.cpu().numpy())\n",
        "    return np.vstack(features)\n",
        "\n",
        "# Get query image\n",
        "query_img = dataset['real_studio_flat']['query'][0]['image']\n",
        "\n",
        "# Get a few gallery images\n",
        "gallery_imgs = [dataset['real_studio_flat']['gallery'][i]['image'] for i in range(5)]\n",
        "\n",
        "# Extract features\n",
        "print(\"Extracting query features...\")\n",
        "query_feat = extract_features([query_img], model, transform)\n",
        "\n",
        "print(\"Extracting gallery features...\")\n",
        "gallery_feats = extract_features(gallery_imgs, model, transform)\n",
        "\n",
        "# Compute cosine similarity\n",
        "query_norm = query_feat / np.linalg.norm(query_feat, axis=1, keepdims=True)\n",
        "gallery_norm = gallery_feats / np.linalg.norm(gallery_feats, axis=1, keepdims=True)\n",
        "similarities = np.dot(query_norm, gallery_norm.T)[0]\n",
        "\n",
        "print(\"\\nSimilarity scores:\")\n",
        "for i, sim in enumerate(similarities):\n",
        "    print(f\"  Gallery image {i}: {sim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Query and Top Retrievals\n",
        "\n",
        "Let's visualize the query image alongside the gallery images, ranked by similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "fig, axes = plt.subplots(1, 6, figsize=(18, 3))\n",
        "\n",
        "# Show query\n",
        "axes[0].imshow(query_img)\n",
        "axes[0].set_title('Query', fontsize=12, fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Show gallery images with similarity scores\n",
        "for i, (img, sim) in enumerate(zip(gallery_imgs, similarities)):\n",
        "    axes[i+1].imshow(img)\n",
        "    axes[i+1].set_title(f'Gallery {i}\\nSim: {sim:.3f}', fontsize=10)\n",
        "    axes[i+1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\u2705 Quickstart completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Congratulations! You've learned the basics of LookBench. Here's what you can do next:\n",
        "\n",
        "1. **Full Evaluation**: Check out `02_model_evaluation.ipynb` to evaluate models on all metrics\n",
        "2. **Custom Models**: See `03_custom_model.ipynb` to integrate your own models\n",
        "3. **Explore Subsets**: Try different subsets (aigen_studio, real_streetlook, aigen_streetlook)\n",
        "4. **Compare Models**: Evaluate different models (clip, siglip, dinov2)\n",
        "\n",
        "### Useful Links\n",
        "\n",
        "- \ud83d\udcc4 **Paper**: https://arxiv.org/abs/2601.14706\n",
        "- \ud83c\udfe0 **Project Page**: https://serendipityoneinc.github.io/look-bench-page/\n",
        "- \ud83e\udd17 **Dataset**: https://huggingface.co/datasets/srpone/look-bench\n",
        "- \ud83e\udd17 **GR-Lite Model**: https://huggingface.co/srpone/gr-lite\n",
        "- \ud83d\udcbb **GitHub**: https://github.com/SerendipityOneInc/look-bench"
      ]
    }
  ]
}